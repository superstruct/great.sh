use std::path::PathBuf;
use std::sync::Arc;
use std::time::Duration;

use rmcp::handler::server::tool::{ToolCallContext, ToolRouter};
use rmcp::handler::server::wrapper::Parameters;
use rmcp::model::*;
use rmcp::service::{RequestContext, RoleServer};
use rmcp::{tool, tool_router, ErrorData as McpError, ServerHandler, ServiceExt};

use super::backends::BackendConfig;
use super::registry::TaskRegistry;
use super::tools::*;

/// Maximum characters in a synchronous tool response before truncation.
/// Approximately 20K tokens at 4 chars/token.
const MAX_RESPONSE_CHARS: usize = 80_000;

/// Maximum bytes to read from a single file for the `research` tool.
const MAX_FILE_BYTES: usize = 100 * 1024; // 100 KiB

/// The MCP bridge server handler.
///
/// This struct is the rmcp `ServerHandler` implementation. It holds shared
/// state (backends, registry, config) and routes tool calls via the
/// `ToolRouter` generated by the `#[tool_router]` macro.
#[derive(Clone)]
pub struct GreatBridge {
    backends: Arc<Vec<BackendConfig>>,
    default_backend: Option<String>,
    registry: TaskRegistry,
    preset: Preset,
    allowed_dirs: Option<Vec<PathBuf>>,
    auto_approve: bool,
    tool_router: ToolRouter<Self>,
}

#[tool_router]
impl GreatBridge {
    pub fn new(
        backends: Vec<BackendConfig>,
        default_backend: Option<String>,
        registry: TaskRegistry,
        preset: Preset,
        allowed_dirs: Option<Vec<PathBuf>>,
        auto_approve: bool,
    ) -> Self {
        Self {
            backends: Arc::new(backends),
            default_backend,
            registry,
            preset,
            allowed_dirs,
            auto_approve,
            tool_router: Self::tool_router(),
        }
    }

    // -- chat group -------------------------------------------------------

    #[tool(description = "Send a prompt to an AI backend and get a synchronous response")]
    async fn prompt(&self, params: Parameters<PromptParams>) -> Result<CallToolResult, McpError> {
        let backend = self.resolve_backend(params.0.backend.as_deref())?;
        let (binary, args) = super::backends::build_command_args(
            backend,
            &params.0.prompt,
            params.0.model.as_deref(),
            None,
            self.auto_approve,
        );

        match self.run_sync(&binary, &args).await {
            Ok(output) => {
                let text = truncate_output(&output);
                Ok(CallToolResult::success(vec![Content::text(text)]))
            }
            Err(e) => Ok(CallToolResult::error(vec![Content::text(e)])),
        }
    }

    // -- agent group ------------------------------------------------------

    #[tool(
        description = "Spawn an asynchronous AI backend task. Returns a task_id for later retrieval."
    )]
    async fn run(&self, params: Parameters<RunParams>) -> Result<CallToolResult, McpError> {
        let backend = self.resolve_backend(params.0.backend.as_deref())?;
        let timeout = params.0.timeout_secs.map(Duration::from_secs);

        match self
            .registry
            .spawn_task(backend, &params.0.prompt, timeout, None, None)
            .await
        {
            Ok(task_id) => {
                let json = serde_json::json!({"task_id": task_id});
                Ok(CallToolResult::success(vec![Content::text(
                    json.to_string(),
                )]))
            }
            Err(e) => Ok(CallToolResult::error(vec![Content::text(e.to_string())])),
        }
    }

    #[tool(
        description = "Wait for one or more async tasks to complete. Returns results when all finish or timeout."
    )]
    async fn wait(&self, params: Parameters<WaitParams>) -> Result<CallToolResult, McpError> {
        let timeout = Duration::from_secs(params.0.timeout_secs.unwrap_or(300));
        let snapshots = self
            .registry
            .wait_for_tasks(&params.0.task_ids, timeout)
            .await;
        let json = serde_json::to_string(&snapshots).unwrap_or_else(|_| "[]".to_string());
        Ok(CallToolResult::success(vec![Content::text(json)]))
    }

    #[tool(description = "List all tracked tasks with their current status.")]
    async fn list_tasks(&self) -> Result<CallToolResult, McpError> {
        let snapshots = self.registry.list_tasks().await;
        let json = serde_json::to_string(&snapshots).unwrap_or_else(|_| "[]".to_string());
        Ok(CallToolResult::success(vec![Content::text(json)]))
    }

    #[tool(description = "Get the result of a completed async task.")]
    async fn get_result(
        &self,
        params: Parameters<GetResultParams>,
    ) -> Result<CallToolResult, McpError> {
        match self.registry.get_task(&params.0.task_id).await {
            Some(snapshot) => {
                let json = serde_json::to_string(&snapshot).unwrap_or_else(|_| "{}".to_string());
                Ok(CallToolResult::success(vec![Content::text(json)]))
            }
            None => Ok(CallToolResult::error(vec![Content::text(format!(
                "task {} not found",
                params.0.task_id
            ))])),
        }
    }

    #[tool(description = "Kill a running async task.")]
    async fn kill_task(
        &self,
        params: Parameters<KillTaskParams>,
    ) -> Result<CallToolResult, McpError> {
        match self.registry.kill_task(&params.0.task_id).await {
            Ok(()) => Ok(CallToolResult::success(vec![Content::text("killed")])),
            Err(e) => Ok(CallToolResult::error(vec![Content::text(e.to_string())])),
        }
    }

    // -- research group ---------------------------------------------------

    #[tool(
        description = "Research a topic, optionally with file context. Sends a composite prompt to an AI backend."
    )]
    async fn research(
        &self,
        params: Parameters<ResearchParams>,
    ) -> Result<CallToolResult, McpError> {
        let backend = self.resolve_backend(params.0.backend.as_deref())?;

        // Build composite prompt with file context
        let mut composite_prompt = String::new();
        if let Some(files) = &params.0.files {
            for path in files {
                if let Err(e) = self.validate_path(path) {
                    return Ok(CallToolResult::error(vec![Content::text(e)]));
                }
                match std::fs::read(path) {
                    Ok(bytes) => {
                        let content = if bytes.len() > MAX_FILE_BYTES {
                            let truncated = String::from_utf8_lossy(&bytes[..MAX_FILE_BYTES]);
                            format!("{}\n[truncated at {} bytes]", truncated, MAX_FILE_BYTES)
                        } else {
                            String::from_utf8_lossy(&bytes).to_string()
                        };
                        composite_prompt
                            .push_str(&format!("--- FILE: {} ---\n{}\n\n", path, content));
                    }
                    Err(e) => {
                        composite_prompt
                            .push_str(&format!("--- FILE: {} --- (error: {})\n\n", path, e));
                    }
                }
            }
        }
        composite_prompt.push_str(&params.0.query);

        let (binary, args) = super::backends::build_command_args(
            backend,
            &composite_prompt,
            params.0.model.as_deref(),
            None,
            self.auto_approve,
        );

        match self.run_sync(&binary, &args).await {
            Ok(output) => {
                let text = truncate_output(&output);
                Ok(CallToolResult::success(vec![Content::text(text)]))
            }
            Err(e) => Ok(CallToolResult::error(vec![Content::text(e)])),
        }
    }

    // -- analysis group ---------------------------------------------------

    #[tool(
        description = "Analyze code: review, explain, optimize, audit for security, or generate tests."
    )]
    async fn analyze_code(
        &self,
        params: Parameters<AnalyzeCodeParams>,
    ) -> Result<CallToolResult, McpError> {
        let backend = self.resolve_backend(params.0.backend.as_deref())?;

        // Resolve code_or_path
        let code = if std::path::Path::new(&params.0.code_or_path).exists() {
            if let Err(e) = self.validate_path(&params.0.code_or_path) {
                return Ok(CallToolResult::error(vec![Content::text(e)]));
            }
            match std::fs::read_to_string(&params.0.code_or_path) {
                Ok(content) => content,
                Err(e) => {
                    return Ok(CallToolResult::error(vec![Content::text(format!(
                        "failed to read file {}: {}",
                        params.0.code_or_path, e
                    ))]));
                }
            }
        } else {
            params.0.code_or_path.clone()
        };

        let prompt = format!("{}{}", params.0.analysis_type.instruction_prefix(), code);

        let (binary, args) = super::backends::build_command_args(
            backend,
            &prompt,
            params.0.model.as_deref(),
            None,
            self.auto_approve,
        );

        match self.run_sync(&binary, &args).await {
            Ok(output) => {
                let text = truncate_output(&output);
                Ok(CallToolResult::success(vec![Content::text(text)]))
            }
            Err(e) => Ok(CallToolResult::error(vec![Content::text(e)])),
        }
    }

    // -- subagent group ---------------------------------------------------

    #[tool(
        description = "Spawn an isolated AI subagent with a custom system prompt. Returns a task_id."
    )]
    async fn clink(&self, params: Parameters<ClinkParams>) -> Result<CallToolResult, McpError> {
        let backend = self.resolve_backend(params.0.backend.as_deref())?;

        match self
            .registry
            .spawn_task(
                backend,
                &params.0.prompt,
                None,
                params.0.model.as_deref(),
                Some(&params.0.system_prompt),
            )
            .await
        {
            Ok(task_id) => {
                let json = serde_json::json!({"task_id": task_id});
                Ok(CallToolResult::success(vec![Content::text(
                    json.to_string(),
                )]))
            }
            Err(e) => Ok(CallToolResult::error(vec![Content::text(e.to_string())])),
        }
    }
}

// -- ServerHandler implementation -----------------------------------------

impl ServerHandler for GreatBridge {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            protocol_version: ProtocolVersion::LATEST,
            capabilities: ServerCapabilities::builder().enable_tools().build(),
            server_info: Implementation {
                name: "great-mcp-bridge".to_string(),
                version: env!("CARGO_PKG_VERSION").to_string(),
                title: Some("great.sh MCP Bridge".to_string()),
                ..Default::default()
            },
            instructions: Some(
                "MCP bridge to AI CLI backends (gemini, codex, claude, grok, ollama). \
                 Use 'prompt' for synchronous queries, 'run'/'wait' for async tasks."
                    .to_string(),
            ),
        }
    }

    /// Filter tool list by preset. Clients only see tools included in the
    /// active preset.
    fn list_tools(
        &self,
        _request: Option<PaginatedRequestParams>,
        _context: RequestContext<RoleServer>,
    ) -> impl std::future::Future<Output = Result<ListToolsResult, McpError>> + Send + '_ {
        let allowed = self.preset.tool_names();
        let all_tools = self.tool_router.list_all();
        let filtered: Vec<Tool> = all_tools
            .into_iter()
            .filter(|t| allowed.contains(&t.name.as_ref()))
            .collect();
        std::future::ready(Ok(ListToolsResult {
            meta: None,
            tools: filtered,
            next_cursor: None,
        }))
    }

    /// Dispatch tool calls via the ToolRouter. Tools not in the preset are
    /// still registered in the router, so they return "tool not found" if
    /// called directly (the client should not see them in list_tools).
    async fn call_tool(
        &self,
        request: CallToolRequestParams,
        context: RequestContext<RoleServer>,
    ) -> Result<CallToolResult, McpError> {
        let context = ToolCallContext::new(self, request, context);
        self.tool_router.call(context).await
    }

    fn get_tool(&self, name: &str) -> Option<Tool> {
        self.tool_router.get(name).cloned()
    }
}

// -- Private helpers ------------------------------------------------------

impl GreatBridge {
    /// Resolve which backend to use from an optional name.
    fn resolve_backend(&self, name: Option<&str>) -> Result<&BackendConfig, McpError> {
        match name {
            Some(n) => self.backends.iter().find(|b| b.name == n).ok_or_else(|| {
                McpError::invalid_params(
                    format!(
                        "backend '{}' not available. Available: {}",
                        n,
                        self.backends
                            .iter()
                            .map(|b| b.name)
                            .collect::<Vec<_>>()
                            .join(", ")
                    ),
                    None,
                )
            }),
            None => {
                // Try default_backend, then first available
                if let Some(ref default) = self.default_backend {
                    self.backends.iter().find(|b| b.name == default.as_str())
                } else {
                    self.backends.first()
                }
                .ok_or_else(|| McpError::invalid_params("no backends available".to_string(), None))
            }
        }
    }

    /// Validate that a file path is allowed by the configured allowed_dirs.
    ///
    /// When `allowed_dirs` is `None`, all paths are allowed (single-user
    /// threat model). When `Some`, each requested path is canonicalized
    /// and checked against the allowed directory prefixes.
    fn validate_path(&self, raw_path: &str) -> Result<(), String> {
        let allowed = match &self.allowed_dirs {
            Some(dirs) => dirs,
            None => return Ok(()),
        };

        let canonical = std::fs::canonicalize(raw_path)
            .map_err(|e| format!("cannot resolve path '{}': {}", raw_path, e))?;

        for dir in allowed {
            if canonical.starts_with(dir) {
                return Ok(());
            }
        }

        Err(format!(
            "path not in allowed directories: '{}' (canonical: {}). \
             Allowed: {}",
            raw_path,
            canonical.display(),
            allowed
                .iter()
                .map(|d| d.display().to_string())
                .collect::<Vec<_>>()
                .join(", "),
        ))
    }

    /// Execute a backend command synchronously (with timeout).
    async fn run_sync(&self, binary: &str, args: &[String]) -> Result<String, String> {
        let mut cmd = tokio::process::Command::new(binary);
        cmd.args(args)
            .stdout(std::process::Stdio::piped())
            .stderr(std::process::Stdio::piped())
            .kill_on_drop(true);

        let child = cmd.spawn().map_err(|e| format!("spawn failed: {}", e))?;

        let timeout = self.registry.default_timeout;
        match tokio::time::timeout(timeout, child.wait_with_output()).await {
            Ok(Ok(output)) => {
                if output.status.success() {
                    Ok(String::from_utf8_lossy(&output.stdout).to_string())
                } else {
                    let stdout = String::from_utf8_lossy(&output.stdout);
                    let stderr = String::from_utf8_lossy(&output.stderr);
                    Err(format!(
                        "exit code {}: {}\n{}",
                        output.status.code().unwrap_or(-1),
                        stdout,
                        stderr,
                    ))
                }
            }
            Ok(Err(e)) => Err(format!("process error: {}", e)),
            Err(_) => Err(format!("timeout after {}s", timeout.as_secs())),
        }
    }
}

/// Truncate output to MAX_RESPONSE_CHARS, appending a notice if truncated.
/// Uses char_indices to find a safe UTF-8 boundary, avoiding panics on
/// multi-byte characters (CJK, emoji, accented text from backends).
fn truncate_output(text: &str) -> String {
    if text.len() > MAX_RESPONSE_CHARS {
        // Find a safe char boundary at or before MAX_RESPONSE_CHARS bytes.
        let end = text
            .char_indices()
            .take_while(|(i, _)| *i <= MAX_RESPONSE_CHARS)
            .last()
            .map(|(i, c)| i + c.len_utf8())
            .unwrap_or(text.len());
        let mut result = text[..end].to_string();
        result.push_str(
            "\n\n[output truncated at 80,000 chars -- use `run`/`wait` for full async output]",
        );
        result
    } else {
        text.to_string()
    }
}

/// Start the bridge server on stdio. This is the main entry point
/// called by `cli/mcp_bridge.rs`.
pub async fn start_bridge(
    backends: Vec<BackendConfig>,
    default_backend: Option<String>,
    registry: TaskRegistry,
    preset: Preset,
    allowed_dirs: Option<Vec<PathBuf>>,
    auto_approve: bool,
) -> anyhow::Result<()> {
    // Canonicalize allowed_dirs at startup so relative paths work
    let allowed_dirs = allowed_dirs.map(|dirs| {
        let resolved: Vec<PathBuf> = dirs
            .into_iter()
            .filter_map(|d| {
                std::fs::canonicalize(&d)
                    .map_err(|e| {
                        tracing::warn!(
                            "allowed_dirs: cannot resolve '{}': {} (skipping)",
                            d.display(),
                            e
                        );
                        e
                    })
                    .ok()
            })
            .collect();
        if resolved.is_empty() {
            tracing::warn!("allowed_dirs: resolved to empty list; all file reads will be rejected");
        }
        resolved
    });

    let bridge = GreatBridge::new(
        backends,
        default_backend,
        registry.clone(),
        preset,
        allowed_dirs,
        auto_approve,
    );

    tracing::info!(
        "great-mcp-bridge starting (preset={:?}, backends={})",
        preset,
        bridge
            .backends
            .iter()
            .map(|b| b.name)
            .collect::<Vec<_>>()
            .join(","),
    );

    let service = bridge
        .serve(rmcp::transport::io::stdio())
        .await
        .map_err(|e| anyhow::anyhow!("failed to start MCP bridge server: {}", e))?;

    service
        .waiting()
        .await
        .map_err(|e| anyhow::anyhow!("bridge server error: {}", e))?;

    // Socrates concern #13: ensure all spawned processes are cleaned up
    registry.shutdown_all().await;

    Ok(())
}
